{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "814f9fe1-589d-41bb-a801-06081ca4319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision.io import ImageReadMode\n",
    "from torchvision import transforms\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import scipy.signal as spsig\n",
    "import scipy.signal.windows as spwin\n",
    "\n",
    "import random\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from torchvision import io\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b187fe4-4099-48a0-b782-5621d948f74b",
   "metadata": {},
   "source": [
    "## Generate Spectrograms\n",
    "Basically convert a .wav or .mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe72a760-ad5a-4240-a3a4-f7d82a090595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/id3.c:process_extra():681] error: No extra frame text / valid description?\n",
      "[src/libmpg123/id3.c:process_extra():681] error: No extra frame text / valid description?\n",
      "[src/libmpg123/id3.c:process_extra():681] error: No extra frame text / valid description?\n",
      "[src/libmpg123/id3.c:process_extra():681] error: No extra frame text / valid description?\n",
      "[src/libmpg123/id3.c:process_extra():681] error: No extra frame text / valid description?\n",
      "[src/libmpg123/id3.c:process_extra():681] error: No extra frame text / valid description?\n",
      "[src/libmpg123/id3.c:process_extra():681] error: No extra frame text / valid description?\n",
      "[src/libmpg123/id3.c:process_extra():681] error: No extra frame text / valid description?\n",
      "[src/libmpg123/id3.c:process_extra():681] error: No extra frame text / valid description?\n",
      "[src/libmpg123/id3.c:process_extra():681] error: No extra frame text / valid description?\n",
      "[src/libmpg123/id3.c:process_extra():681] error: No extra frame text / valid description?\n",
      "[src/libmpg123/id3.c:process_extra():681] error: No extra frame text / valid description?\n"
     ]
    }
   ],
   "source": [
    "# Create spectrograms\n",
    "def generateSpectrograms(source_dir, out_dir, window, hop):\n",
    "\n",
    "# array of sound types available\n",
    "    sound_types = [\"Snare\", \"Trumpet\", \"Violin\"]\n",
    "\n",
    "    # iterate over each of the sound types\n",
    "    for sound_type in sound_types:\n",
    "\n",
    "        # establish output directories\n",
    "        # TODO: Add checks that directorties exist\n",
    "        sound_dir = os.fsencode(source_dir + \"/\" + sound_type)\n",
    "        spec_dir = os.fsencode(out_dir + \"/\" + sound_type)\n",
    "\n",
    "        if not os.path.exists(spec_dir):\n",
    "            os.makedirs(spec_dir)\n",
    "        \n",
    "        # remove all existing spectrograms\n",
    "        for spec_file in os.listdir(spec_dir):\n",
    "            filename = os.fsdecode(spec_file)\n",
    "            if filename.endswith(\".png\"):\n",
    "                os.remove(os.path.join(spec_dir, spec_file))\n",
    "\n",
    "\n",
    "        # iterate over each file (in each directory)\n",
    "        for sound_file in os.listdir(sound_dir):\n",
    "            filename = os.fsdecode(sound_file)\n",
    "\n",
    "            # filter out audio files\n",
    "            if filename.endswith(\".wav\") or filename.endswith(\".mp3\" or filename.endswith(\".flac\")):\n",
    "                \n",
    "                # load sound files\n",
    "                # librosa is convenient because it resamples and downscales to mono automatically\n",
    "                y_orig, fs = librosa.load(os.path.join(sound_dir, sound_file), mono=True, sr=48000)\n",
    "\n",
    "                # set output time and corresponding number of samples\n",
    "                output_time = 1 # seconds\n",
    "                output_len = output_time * fs\n",
    "\n",
    "                # number of samples of loaded file\n",
    "                input_len = np.shape(y_orig)[0]\n",
    "\n",
    "                # skip if loaded file is too short analyse\n",
    "                if input_len < output_len:\n",
    "                    print(filename, \"is too short, skipping conversion\")\n",
    "                    continue\n",
    "                \n",
    "                # find a section with a high rms value\n",
    "                jump = output_len//2 # spacing between sections\n",
    "                rms_best = 0\n",
    "                rms_best_start = 0\n",
    "\n",
    "                # loop over sections of the sample to find the big with the best rms value\n",
    "                for start in range(0, input_len - output_len, jump):\n",
    "                    end = start + output_len\n",
    "                    rms = np.sqrt(np.mean(np.square(y_orig[start:end])))\n",
    "                    if rms > rms_best:\n",
    "                        rms_best = rms\n",
    "                        rms_best_start = start\n",
    "\n",
    "                # window with best RMS value\n",
    "                y = y_orig[rms_best_start:rms_best_start + output_len]\n",
    "\n",
    "                # Compute the Short-Time Fourier Transform (STFT)\n",
    "                # D = librosa.stft(y)\n",
    "\n",
    "                # STFT calc for spectrogramss\n",
    "\n",
    "                T_x, N = 1 / fs, output_len  # 20 Hz sampling rate for 50 s signal\n",
    "\n",
    "                t_x = np.arange(N) * T_x  # time indexes for signal\n",
    "\n",
    "                # SFT = spsig.ShortTimeFFT(win, hop=hop, fs=fs, mfft=16000, scale_to='psd')\n",
    "                SFT = spsig.ShortTimeFFT(window, hop=hop, fs=fs, mfft=4096, scale_to='psd')\n",
    "\n",
    "                Sx2 = SFT.spectrogram(y)  # calculate absolute square of STFT\n",
    "\n",
    "                Sx_dB = 10 * np.log10(np.fmax(Sx2, 5e-9))  # limit range to ~-83dB\n",
    "\n",
    "                fig1, ax1 = plt.subplots(figsize=(6., 4.))  # enlarge plot a bit\n",
    "                im1 = ax1.imshow(Sx_dB, origin='lower', aspect='auto', extent=SFT.extent(N), cmap='gray')\n",
    "\n",
    "\n",
    "                output_file = os.path.join(spec_dir, os.fsencode(os.path.splitext(filename)[0] + \".png\"))\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                plt.savefig(output_file, bbox_inches='tight', pad_inches=0)        \n",
    "                plt.close()\n",
    "\n",
    "rectangular = np.ones(1024)\n",
    "\n",
    "generateSpectrograms(\"../Training-data/Sounds\", \"../Training-data/Rectangular\", rectangular, 512)\n",
    "generateSpectrograms(\"../Test-data/Sounds\", \"../Test-data/Rectangular\", rectangular, 512)\n",
    "\n",
    "hamming = spwin.hamming(1024)\n",
    "\n",
    "generateSpectrograms(\"../Training-data/Sounds\", \"../Training-data/Hamming\", hamming, 512)\n",
    "generateSpectrograms(\"../Test-data/Sounds\", \"../Test-data/Hamming\", hamming, 512)\n",
    "\n",
    "blackman = spwin.blackman(1024)\n",
    "\n",
    "generateSpectrograms(\"../Training-data/Sounds\", \"../Training-data/Blackman\", blackman, 512)\n",
    "generateSpectrograms(\"../Test-data/Sounds\", \"../Test-data/Blackman\", blackman, 512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b246693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays of ground truth data\n",
    "\n",
    "def generateGroundTruthCSV(specdir, groundTruthCSVName):\n",
    "\n",
    "    sound_types = [\"Snare\", \"Trumpet\", \"Violin\"]\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for sound_type in sound_types:\n",
    "\n",
    "        # establish output directories\n",
    "        # TODO: Add checks that directorties exist\n",
    "        spec_dir = os.fsencode(specdir + \"/\" + sound_type)\n",
    "        \n",
    "        # remove all existing spectrograms\n",
    "        for spec_file in os.listdir(spec_dir):\n",
    "            \n",
    "            filename = os.fsdecode(spec_file)\n",
    "            \n",
    "            if filename.endswith(\".png\"):\n",
    "\n",
    "                data.append([os.fsdecode(os.path.join(spec_dir, spec_file)), sound_type])\n",
    "\n",
    "    key = lambda row: row[0] \n",
    "\n",
    "    data.sort(key=lambda item: item[0])\n",
    "\n",
    "    with open(groundTruthCSVName, 'w+') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerows(data)\n",
    "\n",
    "generateGroundTruthCSV(\"../Training-data/Rectangular\", \"../Training-data/rectangular.csv\")\n",
    "generateGroundTruthCSV(\"../Training-data/Hamming\", \"../Training-data/hamming.csv\")\n",
    "generateGroundTruthCSV(\"../Training-data/Blackman\", \"../Training-data/blackman.csv\")\n",
    "\n",
    "generateGroundTruthCSV(\"../Test-data/Rectangular\", \"../Test-data/rectangular.csv\")\n",
    "generateGroundTruthCSV(\"../Test-data/Hamming\", \"../Test-data/hamming.csv\")\n",
    "generateGroundTruthCSV(\"../Test-data/Blackman\", \"../Test-data/blackman.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daee304",
   "metadata": {},
   "source": [
    "## Define classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9b7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"Snare\", \"Trumpet\", \"Violin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf945f-4365-4621-a6e6-99cdafaf9c7b",
   "metadata": {},
   "source": [
    "## Create dataset class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "140b7766-38e1-4ec7-88b1-f58eb59963e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pilla\\AppData\\Local\\Temp\\ipykernel_24088\\2176705582.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class InstrumentsDataset(Dataset):\n",
    "    \"\"\"Instruments dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.instrument_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instrument_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.instrument_frame.iloc[idx, 0])\n",
    "        # image = io.read_image(img_name, mode=ImageReadMode.GRAY)\n",
    "        # image = io.read_image(img_name)\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        \n",
    "        instrument = self.instrument_frame.iloc[idx, 1]\n",
    "        instrument_id = classes.index(instrument)\n",
    "\n",
    "        return image, instrument_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd6d2f",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8030ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, numChannels,numClasses):\n",
    "  \n",
    "        super(Model, self).__init__()\n",
    "        #First layer (convolutional)\n",
    "        self.conv1 = nn.Conv2d(numChannels, 6, 3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Second layer (also convolutional)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.LazyLinear(120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.LazyLinear(numClasses)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = flatten(x, start_dim=1, end_dim=-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.logsoftmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493ed7e",
   "metadata": {},
   "source": [
    "## Create datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "52cea728-4851-48a3-aa7a-f131b68eb44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose(\n",
    "#     [transforms.Grayscale(),\n",
    "#      transforms.ToTensor(),\n",
    "#      transforms.Normalize((0.5), (0.5)),\n",
    "#      transforms.Resize((64, 64))])\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "    # transforms.Resize((64, 64))\n",
    "    ])\n",
    "\n",
    "train_val_dataset = InstrumentsDataset(csv_file = \"../Training-data/blackman.csv\", root_dir = \".\", transform=trans)\n",
    "test_dataset = InstrumentsDataset(csv_file = \"../Test-data/blackman.csv\", root_dir = \".\", transform = trans)\n",
    "\n",
    "train_size = int(0.8 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44c8a6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.2908, 0.3193, 0.3050,  ..., 0.3779, 0.3728, 0.3181],\n",
      "         [0.4043, 0.4170, 0.4213,  ..., 0.4659, 0.4229, 0.3139],\n",
      "         [0.7088, 0.6894, 0.6708,  ..., 0.5156, 0.5531, 0.4057]]])\n",
      "torch.Size([1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "image, instrument_id = train_dataset[0]\n",
    "\n",
    "print(image)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67089a59",
   "metadata": {},
   "source": [
    "## Go and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9c04ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/venv/lib/python3.11/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 1\n",
      "# mini-batch 5\n",
      "train loss: 0.2046717071533203\n",
      "validation loss: 1.2205586830774944 validation accuracy: 0.5128205128205128\n",
      "\n",
      "# mini-batch 10\n",
      "train loss: 0.06070639669895172\n",
      "validation loss: 0.9739670753479004 validation accuracy: 0.38461538461538464\n",
      "\n",
      "Starting Epoch: 2\n",
      "# mini-batch 5\n",
      "train loss: 0.03966433107852936\n",
      "validation loss: 0.6906323035558065 validation accuracy: 0.6923076923076923\n",
      "\n",
      "# mini-batch 10\n",
      "train loss: 0.03739737987518311\n",
      "validation loss: 0.6232115228970846 validation accuracy: 0.5897435897435898\n",
      "\n",
      "Starting Epoch: 3\n",
      "# mini-batch 5\n",
      "train loss: 0.03158049881458282\n",
      "validation loss: 0.538288950920105 validation accuracy: 0.717948717948718\n",
      "\n",
      "# mini-batch 10\n",
      "train loss: 0.02888959139585495\n",
      "validation loss: 0.5607175926367441 validation accuracy: 0.717948717948718\n",
      "\n",
      "Starting Epoch: 4\n",
      "# mini-batch 5\n",
      "train loss: 0.024217396676540374\n",
      "validation loss: 0.5731888711452484 validation accuracy: 0.6153846153846154\n",
      "\n",
      "# mini-batch 10\n",
      "train loss: 0.024933967888355255\n",
      "validation loss: 0.554222176472346 validation accuracy: 0.6923076923076923\n",
      "\n",
      "Starting Epoch: 5\n",
      "# mini-batch 5\n",
      "train loss: 0.020790665447711944\n",
      "validation loss: 0.532027006149292 validation accuracy: 0.7435897435897436\n",
      "\n",
      "# mini-batch 10\n",
      "train loss: 0.022227032780647277\n",
      "validation loss: 0.589601476987203 validation accuracy: 0.6666666666666666\n",
      "\n",
      "Starting Epoch: 6\n",
      "# mini-batch 5\n",
      "train loss: 0.02279256731271744\n",
      "validation loss: 0.5230880479017893 validation accuracy: 0.717948717948718\n",
      "\n",
      "# mini-batch 10\n",
      "train loss: 0.023204502165317536\n",
      "validation loss: 0.5152603387832642 validation accuracy: 0.717948717948718\n",
      "\n",
      "Starting Epoch: 7\n",
      "# mini-batch 5\n",
      "train loss: 0.022135196924209594\n",
      "validation loss: 0.5952736039956411 validation accuracy: 0.5897435897435898\n",
      "\n",
      "# mini-batch 10\n",
      "train loss: 0.01966785341501236\n",
      "validation loss: 0.5247966249783834 validation accuracy: 0.717948717948718\n",
      "\n",
      "Starting Epoch: 8\n",
      "# mini-batch 5\n",
      "train loss: 0.01801737993955612\n",
      "validation loss: 0.5997833410898844 validation accuracy: 0.6410256410256411\n",
      "\n",
      "# mini-batch 10\n",
      "train loss: 0.016262629926204683\n",
      "validation loss: 0.6924560268719991 validation accuracy: 0.6153846153846154\n",
      "\n",
      "Starting Epoch: 9\n",
      "# mini-batch 5\n",
      "train loss: 0.014552545845508575\n",
      "validation loss: 0.7579005062580109 validation accuracy: 0.6410256410256411\n",
      "\n",
      "# mini-batch 10\n",
      "train loss: 0.013886154890060424\n",
      "validation loss: 0.8630419174830118 validation accuracy: 0.6410256410256411\n",
      "\n",
      "Starting Epoch: 10\n",
      "# mini-batch 5\n",
      "train loss: 0.011744241192936898\n",
      "validation loss: 0.7542526920636495 validation accuracy: 0.6153846153846154\n",
      "\n",
      "# mini-batch 10\n",
      "train loss: 0.015039925724267959\n",
      "validation loss: 1.0170819262663524 validation accuracy: 0.5897435897435898\n",
      "\n",
      "Starting Epoch: 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     48\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[78], line 23\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(x)\n\u001b[1;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool1(x)\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Model(1, len(classes)) # 1 channel for grayscale at this stage\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    # initialise evaluation parameters\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad(): # evaluating so don't produce gradients\n",
    "        for data in loader:\n",
    "            inputs, labels = data # get data from dataloader\n",
    "            outputs = model(inputs) # predict outputs\n",
    "            loss = criterion(outputs, labels) # calculate current loss\n",
    "            _, predicted = torch.max(outputs.data, 1) # calculate predicted data\n",
    "            total += labels.size(0) # total number of labels in the current batch\n",
    "            correct += (predicted == labels).sum().item() # number of labels that are correct\n",
    "            \n",
    "            running_loss += loss.item() # loss? not 100% sure\n",
    "        \n",
    "    # Return mean loss, accuracy\n",
    "    return running_loss / len(loader), correct / total\n",
    "\n",
    "loss_history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': []\n",
    "}\n",
    "\n",
    "\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    print(\"Starting Epoch: {}\".format(epoch+1))\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        model.train()\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 5 == 4:    # print every 5 mini-batches\n",
    "            mean_loss = running_loss / 100\n",
    "            loss_history['train_loss'].append(mean_loss)\n",
    "            print('# mini-batch {}\\ntrain loss: {}'.format(\n",
    "                  i + 1, mean_loss))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            # evaluate on validation dataset\n",
    "            mean_loss, val_acc = evaluate(model, val_loader)\n",
    "            loss_history['val_loss'].append(mean_loss)\n",
    "                  \n",
    "            print(\"validation loss: {} validation accuracy: {}\\n\".format(mean_loss, val_acc))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "# Notes for tomorrow:\n",
    "# Check what type of data is being input / output\n",
    "# Look at a tutorial on custom pytorch image recognition?\n",
    "# https://medium.com/analytics-vidhya/implementing-cnn-in-pytorch-with-custom-dataset-and-transfer-learning-1864daac14cc\n",
    "# https://glassboxmedicine.com/2021/02/06/designing-custom-2d-and-3d-cnns-in-pytorch-tutorial-with-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ef49a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
