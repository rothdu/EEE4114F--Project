% ----------------------------------------------------
% Literature Review
% ----------------------------------------------------
% \documentclass[class=report,11pt,crop=false]{standalone}
% \input{../Style/ChapterStyle.tex}
% \input{../FrontMatter/Glossary.tex}
% \begin{document}
% \ifstandalone
% \tableofcontents
% \fi
% % ----------------------------------------------------
\section{Literature Review \label{ch:literature}}
% \epigraph{If you wish to make an apple pie from scratch, you must first invent the universe.}%
%     {\emph{---Carl Sagan}}
\vspace{0.5cm}
% ----------------------------------------------------

\subsection{Introduction}

Music is a big part of the modern world. Without knowing it, each of us has a deep connection to music whether one chooses to listen to or make music. Music is a universal language that connects individuals despite their differences. With the rapid advancements being made in the fields of Machine Learning (ML) and Deep Learning (DL), the ability to classify musical instruments from their sounds is useful in various applications in the music industry. Despite showing promising results, ML and DL are still not as robust or efficient as they should be to be implemented in classification models. This literature review focuses on the methods of classifying instruments with machine learning, specifically with using spectrograms to analyse audio files. The different data capturing procedures and algorithms implanted by researchers are examined, as well as performance evaluation methods are investigated. Once relevant insight is gained, a machine learning-based instrument classification tool is developed that may possibly be deployed into the real world. 

\subsection{Data Collection}

To train a neural network, a large amount of training data is usually required, especially when developing an audio classification task. Researcher may access repositories like GitHub, Kaggle or Hugging Face, which contain many existing datasets that could be sufficient. Alternatively, freesound.org or pixabay.com offer rich reserves of royalty free music suitable to train the model. 
Vimal et al. chose Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) to gather diverse vocal data to train a speech model. Their findings emphasise the importance of choosing a data source with a variety of data samples, to ensure that the resulting model is exposed to a wide variety of training data such that it performs better in the real world. 

\subsection{Feature Extraction} \label{ss: FE}

Feature extraction is a niche field where the method is purely dependant on the type of audio classification task is at hand. Vimal et al. looked at using Mel Frequency Cepstral Coefficient (MFCC) as a method of feature extraction. The reason being that it is best for detection of human voices. Zhang et al. conducted research on environmental sound classification. The audio clips were converted to energy spectrograms by using a short-time Fourier transform (STFT) with a hamming window size dependant on the number of samples and frequency, along with a 50\% overlap. In this specific study, the datasets were limited, so the spectrograms were divided into smaller parts with extra information from the original spectrogram being added before being fed into the classification network. Wu et al. also chose to convert their audio data into spectrograms through applying an STFT on windowed audio pieces, as they conducted research into attention augmented CNNs. 
Shreevathsa et al. conducted research on music instrument recognition. They use both MFCC and Zero Crossing Rate (ZCR) to extract information of what instruments have been played in an audio sample. ZCR is a common method in audio processing. Both ZCR and MFCC are put together for efficient instrument recognition. MFCC is a feature set that describes an instrument by its spectral shape, like how one would handle the characteristics of a human voice. This is very useful when recognising instrument tones, even though MFCC is useful for human voice recognition. ZCR simply measures the rate of sign changes in the signal, and this can be used to differentiate between positive and negative zero crossings. It is useful in both speech recognition and music instrument retrieval. Essid et al. has conducted research on musical instrument recognition and supports the fact that feature extraction needs to resemble a humansâ€™ perception of music. They experiment with using genetic algorithms and inertia ratio maximisation are suitable for musical instrument recognition. Pairwise feature selection is a different approach which optimises the features for class discrimination, while offering efficiency benefits. 

\subsection{Machine Learning Classification Methods} \label{ss: MLCM}

There has been plenty research on different classification methods. Vimal et al. used a range of classification methods, including Random Forest, Decision Trees, and the most relevant and best performing being the Support Vector Machine (SVM). SVM is useful in supervised learning algorithms, which decides on which class a specific pattern belongs to. It is very easy to implement and works well with smaller training datasets. SVM manages to map data to a high dimensional space to make it easy to categorise the data points regardless of their associated linearity. Dhanalakshmi et al. further emphasised that SVMs is fairly accurate when performing general audio classification tasks, with a quoted accuracy of 92\%. Classification tasks were also conducted using Radial Basis Function Neural Networks (RBFNN), with an accuracy of 93\%. RBFNN is also a supervised learning method, that uses radial basis functions as activation functions since this is a feedforward neural network. Olabanjo et al. quoted a 94\% aggregate score for the measure of performance across different classification thresholds. 
Zhang et al. compared their method of Auto-Conditioned Recurrent Neural Networks (ACRNN) with existing methods and sees that their method obtained the highest classification accuracy of 93.7\%, while other models like multi-stream CNN achieve a similar accuracy when using both raw data and spectrogram information, while ACRNN can with only the spectrograms. ACRNN uses a mix of CNN and bidirectional gated recurrent unit layers (bi-GRU) to compute the features and timing of sounds, including eight layers of convolution and two layers of Bi-GRU, with slight tuning made for effective training and regularisation. This method may be excessive when it comes to datasets with less variation, but useful in real world applications. 
Wu et al. proposed a FreqCNN model which combined CNNs and attention mechanisms to learn the features of the input data, i.e., spectrograms. A mix of basic and attention-based convolution blocks split the spectrogram into segments, which allows for the model to examine both local and global variations in features, with an attempt to improving classification accuracy. The model was tested on its ability to handle accent classification, speaker identification, and speech emotion recognition, mostly speech recognition tasks. The proposed model was compared to traditional learning methods. The use of convolution blocks to handle both local frequency areas and global frequency areas may increase the computational accuracy, and in some cases does not improve the accuracy enough to justify implementing it. However, in an instrument classification model, where an instrument may be played at different notes, FreqCNN would make the model more robust for real world applications. 
Shreevathsa et al. utilised CNN as their classification models. This is a neural network where all the layers are not fully connected and use convolution rather than regular multiplication. CNN are also known as Shift Invariant Artificial Neural Networks (SIANN), which is a better version of tradition neural networks since it is controllable. The CNNs would extract features from layers in the input data and connects these features to reduce computational requirements. In testing, they found that CNNs outperformed ANNs in audio classification, while reducing the memory and complexity of the neural network. It does take more time to train and test a CNN model in comparison to ANN. 

\subsection{Performance Evaluation}

Giannoulis et al. reported on their evaluation techniques to test the performance of their instrument recognition model using metrics like precision and recall. The results provide an overview of model performance when different training data is applied with various instruments. 

\subsection{Conclusion}
The literature review provided valuable insight into the current methodologies used in audio classification, particularly for instrument  classification. The review started off with the exploration of different methods of data collection, highlighting the importance of creating a diverse dataset to improve the robustness of the model when deployed in the real-world.  

The common methods of feature extraction, most notably MFCC and ZCR, emerged as useful methods to provide the learning model with relevant information for classification tasks. 

When it came the the ML classification methods, the options were endless, ranging from Random Forest to SVMs. However, CNNs stood out as being the best classification tool for this specific task based on research conducted on similar work. 

Precision and recall were seen as reliable performance metrics to assess the results. 

Overall, the understanding of audio classification, specifically with regards to instruments, has been greatly improved after looking at the current solutions available. 


% ----------------------------------------------------
% \ifstandalone
% \bibliography{../Bibliography/References.bib}
% \printnoidxglossary[type=\acronymtype,nonumberlist]
% \fi
% \end{document}
% ----------------------------------------------------