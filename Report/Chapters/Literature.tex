% ----------------------------------------------------
% Literature Review
% ----------------------------------------------------
\documentclass[class=report,11pt,crop=false]{standalone}
\input{../Style/ChapterStyle.tex}
\input{../FrontMatter/Glossary.tex}
\begin{document}
\ifstandalone
\tableofcontents
\fi
% % ----------------------------------------------------
\section{Literature Review \label{ch:literature}}
% \epigraph{If you wish to make an apple pie from scratch, you must first invent the universe.}%
%     {\emph{---Carl Sagan}}
% ----------------------------------------------------

\subsection{Introduction}

Music is a big part of the modern world. Without knowing it, each of us has a deep connection to music, whether choosing to listen to or make music. Music is a universal language that connects individuals despite their differences. With the rapid advancements being made in the fields of Machine Learning (ML) and Deep Learning (DL), the ability to classify musical instruments from their sounds is useful in various applications in the music industry. Despite showing promising results, the scope for research in  ML and DL is still broad. This literature review focuses on the methods of classifying instruments with machine learning, specifically with using spectrograms and MFCCs to analyse audio files. The different data capturing procedures and algorithms implanted by researchers are examined. Once relevant insight is gained, a machine learning-based instrument classification tool is developed that may possibly be deployed into the real world. 

\subsection{Data Collection}

To train a neural network, a large amount of training data is usually required, especially when developing an audio classification task. Investigations often use large pre-existing datasets, but creating a new dataset poses a more engaging problem. A wealth of royalty free music suitable for model training can be found on sites like \href{https://freesound.org/}{freesound} \cite{freesound} and \href{https://pixabay.com/}{pixabay} \cite{pixabay}.

Vimal et al.  chose Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) to gather diverse vocal data to train a speech model \cite{Vimal_2021}. Their findings emphasise the importance of choosing a data source with a variety of data samples, to ensure that the resulting model is exposed to a wide variety of training data such that it performs better in the real world \cite{Vimal_2021}. 

\subsection{Feature Extraction} \label{ss: FE}

Feature extraction is a niche field where the method is purely dependant on the type of audio classification task is at hand. \cite{Vimal_2021} looked at using MFCCs as a method of feature extraction, the reason being that it is the best for detection of human voices. Zhang et al.  conducted research on environmental sound classification \cite{Zhang2021}. The audio clips were converted to energy spectrograms by using a short-time Fourier transform (STFT) with a hamming window size dependant on the number of samples and frequency, along with a 50\% overlap \cite{Zhang2021}. In this specific study, the datasets were limited, so the spectrograms were divided into smaller parts with extra information from the original spectrogram being added before being fed into the classification network \cite{Zhang2021}. Wu et al. also chose to convert their audio data into spectrograms through applying an STFT on windowed audio pieces, as they conducted research into attention augmented CNNs \cite{Wu_2018}. 

Shreevathsa et al. conducted research on music instrument recognition \cite{Shreevathsa_2020}. They used both MFCCs and Zero Crossing Rate (ZCR) to extract information of what instruments have been played in an audio sample. ZCR is a common method in audio processing \cite{Shreevathsa_2020}. Both ZCR and MFCC are put together for efficient instrument recognition \cite{Shreevathsa_2020}. MFCCs are a feature set that describes an instrument by its spectral shape, like how one would handle the characteristics of a human voice \cite{Shreevathsa_2020}. This is very useful when recognising instrument tones, even though MFCC is typically used for human voice recognition \cite{Shreevathsa_2020}. ZCR simply measures the rate of sign changes in the signal, and this can be used to differentiate between positive and negative zero crossings. It is useful in both speech recognition and music instrument retrieval. Essid et al. have conducted research on musical instrument recognition and supports the fact that feature extraction needs to resemble a humansâ€™ perception of music \cite{Essid_2006}. \cite{Essid_2006}'s experiments using genetic algorithms and inertia ratio maximisation are suitable for musical instrument recognition. Pairwise feature selection is a different approach which optimises the features for class discrimination, while offering efficiency benefits \cite{Essid_2006}. 

\subsection{Machine Learning Classification Methods} \label{ss: MLCM}

There has been plenty research on different classification methods. \cite{Vimal_2021} used a range of classification methods, including Random Forest, Decision Trees, and the most relevant and best performing being the Support Vector Machine (SVM). SVM is useful in supervised learning algorithms, which decides on which class a specific pattern belongs to \cite{Vimal_2021}. It is very easy to implement and works well with smaller training datasets \cite{Vimal_2021}. SVM manages to map data to a high dimensional space to make it easy to categorise the data points regardless of their associated linearity \cite{Vimal_2021}. Dhanalakshmi et al. further emphasised that SVMs are fairly accurate when performing general audio classification tasks, with a quoted accuracy of 92\% \cite{Dhanalakshmi_2009}. Classification tasks were also conducted using Radial Basis Function Neural Networks (RBFNN), with an accuracy of 93\% \cite{Dhanalakshmi_2009}. RBFNN is also a supervised learning method, that uses radial basis functions as activation functions since this is a feedforward neural network \cite{Dhanalakshmi_2009}. Olabanjo et al. quoted a 94\% aggregate score for the measure of performance across different classification thresholds \cite{Olabanjo_2022}. 

Zhang et al. compared their method of Auto-Conditioned Recurrent Neural Networks (ACRNN) with existing methods and saw that their method obtained the highest classification accuracy of 93.7\% \cite{Zhang2021}. Other models like multi-stream CNN achieved a similar accuracy when using both raw data and spectrogram information, while ACRNN can with only the spectrograms \cite{Zhang2021}. ACRNN uses a mix of Convolutional Neural Networks (CNNs) and bidirectional gated recurrent unit layers (bi-GRU) to compute the features and timing of sounds, including eight layers of convolution and two layers of Bi-GRU, with slight tuning made for effective training and regularisation \cite{Zhang2021}. This method may be excessive when it comes to datasets with less variation, but useful in real world applications \cite{Zhang2021}. 

Wu et al. proposed a FreqCNN model which combined CNNs and attention mechanisms to learn the features of the input data, i.e., spectrograms  \cite{Wu_2018}. A mix of basic and attention-based convolution blocks split the spectrogram into segments, which allowed for the model to examine both local and global variations in features  \cite{Wu_2018}. The model was tested on its ability to handle accent classification, speaker identification, and speech emotion recognition, mostly speech recognition tasks  \cite{Wu_2018}. The proposed model was compared to traditional learning methods  \cite{Wu_2018}. The use of convolution blocks to handle both local frequency areas and global frequency areas may increase the computational accuracy, and in some cases does not improve the accuracy enough to justify implementing it  \cite{Wu_2018}. However, in an instrument classification model, where an instrument may be played at different notes, FreqCNN would make the model more robust for real world applications  \cite{Wu_2018}. 

Shreevathsa et al. utilised CNN as their classification models \cite{Shreevathsa_2020}. This is a neural network where all the layers are not fully connected and use convolution rather than regular multiplication \cite{Shreevathsa_2020}. CNN are also known as Shift Invariant Artificial Neural Networks (SIANN), which are a better version of traditional neural networks since they are more controllable \cite{Shreevathsa_2020}.  The CNNs extract features from layers in the input data and connect these features to reduce computational requirements. In testing, \cite{Shreevathsa_2020} found that CNNs outperformed ANNs in audio classification, while reducing the memory and complexity of the neural network. It does take more time to train and test a CNN model in comparison to ANN. 

\subsection{Conclusion}
The literature review provided valuable insight into the current methodologies used in audio classification, particularly for instrument  classification. The review started off with the exploration of different methods of data collection, highlighting the importance of creating a diverse dataset to improve the robustness of the model when deployed in the real-world.  

The common methods of feature extraction, most notably MFCC and ZCR, emerged as useful methods to provide the learning model with relevant information for classification tasks. 

When it came the the ML classification methods, the options were endless, ranging from Random Forest to SVMs. However, CNNs stood out as being the best classification tool for this specific task based on research conducted on similar work.  Overall, the understanding of audio classification, specifically with regards to instruments, has been greatly improved after looking at the current solutions available. 


% ----------------------------------------------------
\ifstandalone
\bibliography{../Bibliography/References.bib}
\printnoidxglossary[type=\acronymtype,nonumberlist]
\fi
\end{document}
% ----------------------------------------------------