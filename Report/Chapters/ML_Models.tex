% ----------------------------------------------------
% Machine Learning Classification Methods
% ----------------------------------------------------
\documentclass[class=report,11pt,crop=false]{standalone}
\input{../Style/ChapterStyle.tex}
\input{../FrontMatter/Glossary.tex}
\begin{document}
\ifstandalone
\tableofcontents
\fi
% % ----------------------------------------------------
\section{Machine Learning Classification Methods \label{ch:ML_Methods}}

% The Artificial Neural Network (ANN) is a computational model that works similarly to the human brain. How it works is that there are numerous connected nodes that each perform a mathematical operation, which affects the respective output. By connecting the nodes together and setting their respective parameters, complex functions can be computed. The ANN requires lots of data since it learns from previous experiences, being able to derive complex relationships between data that to us humans have little to no correlation between sets. Looking at the sound datasets, the ability of an ANN to easily derive non-linear relationships between data could be valuable. 


% The Convolution Neural Network (CNN) is a artificial neural network that is very effective when working with grid data, i.e., 2D arrays. CNNs are built to adapt automatically to the features of the input data, making it very versatile. This is particularly useful in instrument classification since trying to extract features from data was proving to be time inefficient. When dealing with spectrograms, the CNN is very good at handling both time and frequency components simultaneously to analyse patterns and structures which are useful in classification. 

In section \ref{ss: MLCM}, various ML models were looked at, however the one that stood out as being the best suited for this project was the CNN. Shreevathsa et al. \cite{Shreevathsa_2020} verified the CNN performance in a comparative study between the ANN and CNN. The ultimate conclusion was that the CNN worked far better in audio classification, outperforming the ANN. 

Rather than comparing the performance between the ANN and CNN when changing the input features, it was deemed more insightful to analyse the model performance between a shallow and deeper neural network. This was tested by feeding both neural networks with spectrograms with different windowing methods applied and MFCCs with no comparison of windowing, as desribed in section \ref{ch:Feature_Extraction}. 

Three different CNNs were utilised in this investigation. The first was a `shallow' network, consisting of a single convolutional layer, followed by a max-pooling stage and two linear layers. The `deeper' network consisted of two convolutional layers, each of which was followed by a max pooling stages. This was followed by three linear layers. 

The main apples-to-apples comparison of network depth was performed on the `shallow' and `deeper' networks by training the networks on spectrograms as inputs. For the MFCCs, a alternative third network was established in lieu of the `deeper' network. It was very similar to the simple network, but used a horizontal kernel. This was because the vertical features of MFCCs were thought to possibly show limited relational importance, instead, the horizontal (time) axis was likely to contain the most relational importance which ought to be assessed by the kernel. The MFCCs were also significantly lower-resolution images than the spectrograms and already contained sumarised data about large snippets of the original sounds; for this reason the max pooling stage was eliminated from the MFCC network. Additionally, for the MFCCs, both the original `shallow' network and the horizontal-kernel network had to be modified to accept three input channels, since the MFCCs contained meaningful data in their colour spectrum (compared to the spectrograms which were grayscale).

All networks using Rectified Linear Units for their activation functions. Additionally, the final classification used the logarithmic softmax function to identify the class with the highest probability.




% ----------------------------------------------------
\ifstandalone
\bibliography{../Bibliography/References.bib}
\printnoidxglossary[type=\acronymtype,nonumberlist]
\fi
\end{document}
% ----------------------------------------------------