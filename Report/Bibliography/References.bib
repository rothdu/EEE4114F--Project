@INPROCEEDINGS{Giannoulis_2014,
  author={Giannoulis, Dimitrios and Benetos, Emmanouil and Klapuri, Anssi and Plumbley, Mark D.},
  booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Improving instrument recognition in polyphonic music through system integration}, 
  year={2014},
  volume={},
  number={},
  pages={5222-5226},
  keywords={Instruments;Databases;Detectors;Music;Training;Speech;Multiple signal classification;Musical instrument recognition;automatic music transcription;music signal analysis},
  doi={10.1109/ICASSP.2014.6854599}}

@ARTICLE{Essid_2006,
  author={Essid, S. and Richard, G. and David, B.},
  journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
  title={Musical instrument recognition by pairwise classification strategies}, 
  year={2006},
  volume={14},
  number={4},
  pages={1401-1412},
  keywords={Instruments;Support vector machines;Support vector machine classification;Spatial databases;Timbre;Genetic algorithms;Source separation;Music information retrieval;Pattern recognition;Signal processing algorithms;Feature selection;Gaussian mixture model (GMM);genetic algorithms;inertia ratio maximization with feature space projection (IRMFSP);musical instrument recognition;pairwise classification;support vector machine (SVM)},
  doi={10.1109/TSA.2005.860842}}

@INPROCEEDINGS{Shreevathsa_2020,
  author={Shreevathsa, P.K. and Harshith, M. and M., Abhishek Rao and Ashwini},
  booktitle={2020 International Conference on Computation, Automation and Knowledge Management (ICCAKM)}, 
  title={Music Instrument Recognition using Machine Learning Algorithms}, 
  year={2020},
  volume={},
  number={},
  pages={161-166},
  keywords={Instrument Recognition;MFCC;CNN;ANN},
  doi={10.1109/ICCAKM46823.2020.9051514}}

@INPROCEEDINGS{Vimal_2021,
  author={Vimal, B. and Surya, Muthyam and Darshan and Sridhar, V.S. and Ashok, Asha},
  booktitle={2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={MFCC Based Audio Classification Using Machine Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1-4},
  keywords={Support vector machines;Training;Emotion recognition;Machine learning algorithms;Speech recognition;Feature extraction;Classification algorithms;RAVDESS dataset;Emotion recognition;Decission Tree;Support Vector Machine (SVM);Random Forest},
  doi={10.1109/ICCCNT51525.2021.9579881}}

@INPROCEEDINGS{Ren_2014,
  author={Ren, Yuanfang and Wu, Yan},
  booktitle={2014 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Convolutional deep belief networks for feature extraction of EEG signal}, 
  year={2014},
  volume={},
  number={},
  pages={2850-2853},
  keywords={Electroencephalography;Feature extraction;Training;Convolution;Probabilistic logic;Convolutional codes;Accuracy;deep learning;EEG;convolutional deep belief networks;feature learning},
  doi={10.1109/IJCNN.2014.6889383}}

@article{Salvati_2023,
title = {A late fusion deep neural network for robust speaker identification using raw waveforms and gammatone cepstral coefficients},
journal = {Expert Systems with Applications},
volume = {222},
pages = {119750},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119750},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423002518},
author = {Daniele Salvati and Carlo Drioli and Gian Luca Foresti},
keywords = {Speaker identification, Deep neural network, Convolutional neural network, Late fusion, Raw waveform, Gammatone cepstral coefficient},
abstract = {Speaker identification aims at determining the speaker identity by analyzing his voice characteristics, and relies typically on statistical models or machine learning techniques. Frequency-domain features are by far the most used choice to encode the audio input in sound recognition. Recently, some studies have also analyzed the use of time-domain raw waveform (RW) with deep neural network (DNN) architectures. In this paper, we hypothesize that both time-domain and frequency-domain features can be used to increase the robustness of speaker identification task in adverse noisy and reverberation conditions, and we present a method based on a late fusion DNN using RWs and gammatone cepstral coefficients (GTCCs). We analyze the characteristics of RW and spectrum-based short-time features, reporting advantages and limitations, and we show that the joint use can increase the identification accuracy. The proposed late fusion DNN model consists of two independent DNN branches made primarily by convolutional neural networks (CNN) and fully connected neural networks (NN) layers. The two DNN branches have as input short-time RW audio fragments and GTCCs, respectively. The late fusion is computed on the predicted scores of the DNN branches. Since the method is based on short segments, it has the advantage of being independent from the size of the input audio signal, and the identification task can be computed by summing the predicted scores over several short-time frames. Analysis of speaker identification performance computed with simulations show that the late fusion DNN model improves the accuracy rate in adverse noise and reverberation conditions in comparison to the RW, the GTCC, and the mel-frequency cepstral coefficients (MFCCs) features. Experiments with real-world speech datasets confirm the efficiency of the proposed method, especially with small-size audio samples.}
}

@INPROCEEDINGS{Copiaco_2019,
  author={Copiaco, Abigail and Ritz, Christian and Fasciani, Stefano and Abdulaziz, Nidhal},
  booktitle={2019 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)}, 
  title={Scalogram Neural Network Activations with Machine Learning for Domestic Multi-channel Audio Classification}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  keywords={Support vector machines;Feature extraction;Machine learning;Continuous wavelet transforms;Convolutional neural networks;Time-frequency analysis;Machine Learning;deep learning;multi-channel audio;classification;support vector machines},
  doi={10.1109/ISSPIT47144.2019.9001814}}

@article{Wu_2018,
title = {Audio classification using attention-augmented convolutional neural network},
journal = {Knowledge-Based Systems},
volume = {161},
pages = {90-100},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.07.033},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118303848},
author = {Yu Wu and Hua Mao and Zhang Yi},
keywords = {Audio classification, Spectrograms, Convolutional neural networks, Attention mechanism},
abstract = {Audio classification, as a set of important and challenging tasks, groups speech signals according to speakers’ identities, accents, and emotional states. Due to the high dimensionality of the audio data, task-specific hand-crafted features extraction is always required and regarded cumbersome for various audio classification tasks. More importantly, the inherent relationship among features has not been fully exploited. In this paper, the original speech signal is first represented as spectrogram and later be split along the frequency domain to form frequency-distributed spectrogram. This paper proposes a task-independent model, called FreqCNN, to automaticly extract distinctive features from each frequency band by using convolutional kernels. Further more, an attention mechanism is introduced to systematically enhance the features from certain frequency bands. The proposed FreqCNN is evaluated on three publicly available speech databases thorough three independent classification tasks. The obtained results demonstrate superior performance over the state-of-the-art.}
}

@article{Zhang2021,
  title = {Attention based convolutional recurrent neural network for environmental sound classification},
  volume = {453},
  ISSN = {0925-2312},
  url = {http://dx.doi.org/10.1016/j.neucom.2020.08.069},
  DOI = {10.1016/j.neucom.2020.08.069},
  journal = {Neurocomputing},
  publisher = {Elsevier BV},
  author = {Zhang,  Zhichao and Xu,  Shugong and Zhang,  Shunqing and Qiao,  Tianhao and Cao,  Shan},
  year = {2021},
  month = sep,
  pages = {896–903}
}

@inproceedings{Felipe2017,
  title = {Acoustic scene classification using spectrograms},
  url = {http://dx.doi.org/10.1109/SCCC.2017.8405119},
  DOI = {10.1109/sccc.2017.8405119},
  booktitle = {2017 36th International Conference of the Chilean Computer Science Society (SCCC)},
  publisher = {IEEE},
  author = {Felipe,  Gustavo Zanoni and Maldonado,  Yandre and Costa,  Gomes da and Helal,  Lucas Georges},
  year = {2017},
  month = oct 
}

@article{Olabanjo_2022,
title = {A machine learning prediction of academic performance of secondary school students using radial basis function neural network},
journal = {Trends in Neuroscience and Education},
volume = {29},
pages = {100190},
year = {2022},
issn = {2211-9493},
doi = {https://doi.org/10.1016/j.tine.2022.100190},
url = {https://www.sciencedirect.com/science/article/pii/S2211949322000187},
author = {Olusola A. Olabanjo and Ashiribo S. Wusu and Mazzara Manuel},
keywords = {Academic performance, Machine learning, RBFNN},
abstract = {Background
Predictive models for academic performance forecasting have been a useful tool in the improvement of the administrative, counseling and instructional personnel of academic institutions.
Aim
The aim of this work is to develop a Radial Basis Function Neural Network for prediction of students’ performance using their past academic records as well as their cognitive and psychomotor abilities.
Methods
We obtained data from a secondary school repository containing academic, cognitive and psychomotor scores of the students. The preprocessed dataset was used to train the RBFNN model. The impact of Principal Component Analysis on the model performance was also measured.
Results
The results gave a sensitivity (pass prediction) of 93.49%, specificity (failure prediction) of 75%, overall accuracy of 86.59% and an AUC score (aggregate measure of performance across the possible classification thresholds) of 94%.
Conclusion
We established in this study that psychomotor and cognitive abilities also predict students’ performance. This study helps students, parents and teachers to get a projection of academic success even before sitting for the examination.}
}

@article{Dhanalakshmi_2009,
title = {Classification of audio signals using SVM and RBFNN},
journal = {Expert Systems with Applications},
volume = {36},
number = {3, Part 2},
pages = {6069-6075},
year = {2009},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2008.06.126},
url = {https://www.sciencedirect.com/science/article/pii/S0957417408004004},
author = {P. Dhanalakshmi and S. Palanivel and V. Ramalingam},
keywords = {Support vector machines, Radial basis function neural network, Linear predictive coefficients, Linear predictive cepstral coefficients, Mel-frequency cepstral coefficients},
abstract = {In the age of digital information, audio data has become an important part in many modern computer applications. Audio classification has been becoming a focus in the research of audio processing and pattern recognition. Automatic audio classification is very useful to audio indexing, content-based audio retrieval and on-line audio distribution, but it is a challenge to extract the most common and salient themes from unstructured raw audio data. In this paper, we propose effective algorithms to automatically classify audio clips into one of six classes: music, news, sports, advertisement, cartoon and movie. For these categories a number of acoustic features that include linear predictive coefficients, linear predictive cepstral coefficients and mel-frequency cepstral coefficients are extracted to characterize the audio content. Support vector machines are applied to classify audio into their respective classes by learning from training data. Then the proposed method extends the application of neural network (RBFNN) for the classification of audio. RBFNN enables nonlinear transformation followed by linear transformation to achieve a higher dimension in the hidden space. The experiments on different genres of the various categories illustrate the results of classification are significant and effective.}
}
@misc{freesound,
  title = {{freesound}},
  year = {2024},
  howpublished = "\url{https://freesound.org/}",
  note = {[Accessed 16-05-2024]}
}

@misc{pixabay,
  title = {{pixabay}},
  year = {2024},
  howpublished = "\url{https://pixabay.com/}",
  note = {[Accessed 16-05-2024]}
}

@misc{tytel2018Helm,
  author = {Matthew Tytel},
  title = {{Helm}},
  year = {2018},
  howpublished = "\url{https://tytel.org/helm/}",
  note = {[Accessed 16-05-2024]}
}
